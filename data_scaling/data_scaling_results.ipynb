{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8068fad0",
   "metadata": {},
   "source": [
    "## Data Scaling Results on Decagon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3cd196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H' 'I' 'F' 'D' 'C']\n",
      "['H' 'I' 'F' 'D' 'C']\n",
      "Training Length: 24193\n",
      "Testing Length: 6047\n"
     ]
    }
   ],
   "source": [
    "# imports, parameters, environment, etc.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utilities.ogbench_utilities import *\n",
    "from diffusion_planner import DiffusionPlannerConfig\n",
    "from diffusion_planner import eval_model as diffusion_planer_eval\n",
    "from utilities.cleandiffuser.invdynamic import MlpInvDynamic\n",
    "from utilities.cleandiffuser.diffusion import ContinuousDiffusionSDE\n",
    "from utilities.cleandiffuser.diffusion import DiscreteDiffusionSDE\n",
    "from utilities.cleandiffuser.nn_diffusion import JannerUNet1d,CNN1dShiftEq,ConvNext1dShiftEq\n",
    "from utilities.cleandiffuser.nn_condition import MLPCondition\n",
    "from utilities.cleandiffuser.classifier import CumRewClassifier\n",
    "from utilities.cleandiffuser.nn_classifier import HalfJannerUNet1d\n",
    "from utilities.decagon_env import *\n",
    "from utilities.toy_env_utilities import ToyEnvInvDyn\n",
    "from utilities.ogbench_utilities import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from diffusion_planner import get_trajectory\n",
    "\n",
    "\n",
    "env = 'gridland'\n",
    "n_size = 5\n",
    "memory = 1\n",
    "horizon = 512 # for generation so must be power of 2\n",
    "num_envs = 250\n",
    "num_episodes = 1\n",
    "temperature = 0.5\n",
    "n_exec_steps = 512\n",
    "render = True\n",
    "\n",
    "# gets the set of trajectories from the training data \n",
    "import numpy as np \n",
    "config = DiffusionPlannerConfig()\n",
    "config.horizon = 500\n",
    "config.gen_horizon = 512\n",
    "env = Decagon()\n",
    "env.reset()\n",
    "save_directory = 'decagon_data'\n",
    "\n",
    "training_split = np.load(save_directory + '/train_data.npy',allow_pickle=True)\n",
    "testing_split = np.load(save_directory + '/test_data.npy',allow_pickle=True)\n",
    "\n",
    "print(training_split[0]['landmarks'])\n",
    "print(env.get_trajectory_landmarks(training_split[0]['trajectory']))\n",
    "\n",
    "print('Training Length:',len(training_split))\n",
    "print('Testing Length:',len(testing_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80c12d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# In Train: 173\n",
      "# Not In Train: 77\n",
      "# In Test: 35\n",
      "Ratio: 0.308\n"
     ]
    }
   ],
   "source": [
    "# U-Net unconditional, full data\n",
    "\n",
    "training_split = np.load(save_directory + '/train_data.npy',allow_pickle=True)\n",
    "testing_split = np.load(save_directory + '/test_data.npy',allow_pickle=True)\n",
    "config = DiffusionPlannerConfig()\n",
    "config.guidance = 'none'\n",
    "config.num_episodes = 1\n",
    "config.w_cfg = 1.0\n",
    "config.num_envs = num_envs\n",
    "obs_dim = 2\n",
    "fix_mask = torch.zeros((config.gen_horizon, obs_dim)) \n",
    "# --------------- Network Architecture -----------------\n",
    "if config.use_shift_equivariant_arch:\n",
    "    #nn_diffusion = ConvNext1dShiftEq(obs_dim)\n",
    "    nn_diffusion = CNN1dShiftEq(obs_dim,\n",
    "                                kernel_expansion_rate=config.kernel_expansion_rate,\n",
    "                                model_dim = config.model_dim,\n",
    "                                emb_dim = config.emb_dim,\n",
    "                                encode_position = config.add_positional_encoding)\n",
    "else:\n",
    "    nn_diffusion = JannerUNet1d(\n",
    "        obs_dim, model_dim=config.model_dim, emb_dim=config.model_dim, dim_mult=[1, 2, 2, 2],\n",
    "        timestep_emb_type=\"positional\", attention=config.self_attention, kernel_size=5,\n",
    "        use_timestep_emb=config.use_timestep_embeddings)\n",
    "\n",
    "\n",
    "classifier = None\n",
    "# creates condition network if needed (when using classifier-free guidance)\n",
    "if 'cfg' in config.guidance:\n",
    "    nn_condition = MLPCondition(\n",
    "    in_dim=config.lang_enc_size, out_dim=config.emb_dim, hidden_dims=[config.emb_dim, ], act=nn.SiLU(), dropout=config.label_dropout)\n",
    "else:\n",
    "    nn_condition = None\n",
    "loss_weight = torch.ones((config.gen_horizon, obs_dim))\n",
    "agent = DiscreteDiffusionSDE(\n",
    "        nn_diffusion = nn_diffusion, \n",
    "        nn_condition = nn_condition,\n",
    "        classifier = classifier, \n",
    "        fix_mask=fix_mask, \n",
    "        loss_weight=loss_weight, \n",
    "        ema_rate=config.ema_rate,\n",
    "        device=config.device,\n",
    "        diffusion_steps=config.diffusion_steps, \n",
    "        predict_noise=config.predict_noise)\n",
    "\n",
    "# loads agent\n",
    "agent.load('trained_models/DP-UNet-Uncond-Full-decagon-2a272be8diffusion_ckpt_200000.pt')\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "total_not_train = 0\n",
    "# sample language string\n",
    "sampled_trajectories = get_trajectory(agent,config,None)[0]\n",
    "for sample_trajectory in sampled_trajectories:\n",
    "    #sample_trajectory = sampled_trajectories[0][0]\n",
    "    # plots it on the map\n",
    "    #fig,ax = env.get_mpl_plot()\n",
    "    #for i,grid_point in enumerate(sample_trajectory):\n",
    "    #    plt.scatter(grid_point[0],grid_point[1],color=i*np.array((1,1,1))/512,zorder=-1)\n",
    "\n",
    "    # prints out the cooresponding plan string\n",
    "    landmark_tuple = env.get_trajectory_landmarks(sample_trajectory)\n",
    "    #print('Landmark Path:',landmark_tuple)\n",
    "    in_training = False\n",
    "    in_testing = False\n",
    "    for thing in training_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_training = True\n",
    "    for thing in testing_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_testing = True\n",
    "    #if not in_training:\n",
    "    #    print('Not In Training Data!')\n",
    "    if in_training:\n",
    "        total_train += 1\n",
    "    else:\n",
    "        total_not_train += 1\n",
    "    if in_testing:\n",
    "        total_test += 1\n",
    "    #for thing in testing_split:\n",
    "    #    if list(landmark_tuple) == list(thing['landmarks']):\n",
    "    #        print('In Testing Data!')\n",
    "print('# In Train:',total_train)\n",
    "print('# Not In Train:',total_not_train)\n",
    "print('# In Test:',total_test)\n",
    "print('Ratio:',total_not_train/(total_train+total_not_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d6e892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# In Train: 170\n",
      "# Not In Train: 80\n",
      "# In Test: 19\n",
      "Ratio: 0.32\n"
     ]
    }
   ],
   "source": [
    "# U-Net unconditional, medium\n",
    "\n",
    "training_split = np.load(save_directory + '/train_data_medium.npy',allow_pickle=True)\n",
    "config = DiffusionPlannerConfig()\n",
    "config.guidance = 'none'\n",
    "config.num_episodes = 1\n",
    "config.w_cfg = 1.0\n",
    "config.num_envs = num_envs\n",
    "obs_dim = 2\n",
    "fix_mask = torch.zeros((config.gen_horizon, obs_dim)) \n",
    "# --------------- Network Architecture -----------------\n",
    "if config.use_shift_equivariant_arch:\n",
    "    #nn_diffusion = ConvNext1dShiftEq(obs_dim)\n",
    "    nn_diffusion = CNN1dShiftEq(obs_dim,\n",
    "                                kernel_expansion_rate=config.kernel_expansion_rate,\n",
    "                                model_dim = config.model_dim,\n",
    "                                emb_dim = config.emb_dim,\n",
    "                                encode_position = config.add_positional_encoding)\n",
    "else:\n",
    "    nn_diffusion = JannerUNet1d(\n",
    "        obs_dim, model_dim=config.model_dim, emb_dim=config.model_dim, dim_mult=[1, 2, 2, 2],\n",
    "        timestep_emb_type=\"positional\", attention=config.self_attention, kernel_size=5,\n",
    "        use_timestep_emb=config.use_timestep_embeddings)\n",
    "\n",
    "\n",
    "classifier = None\n",
    "# creates condition network if needed (when using classifier-free guidance)\n",
    "if 'cfg' in config.guidance:\n",
    "    nn_condition = MLPCondition(\n",
    "    in_dim=config.lang_enc_size, out_dim=config.emb_dim, hidden_dims=[config.emb_dim, ], act=nn.SiLU(), dropout=config.label_dropout)\n",
    "else:\n",
    "    nn_condition = None\n",
    "loss_weight = torch.ones((config.gen_horizon, obs_dim))\n",
    "agent = DiscreteDiffusionSDE(\n",
    "        nn_diffusion = nn_diffusion, \n",
    "        nn_condition = nn_condition,\n",
    "        classifier = classifier, \n",
    "        fix_mask=fix_mask, \n",
    "        loss_weight=loss_weight, \n",
    "        ema_rate=config.ema_rate,\n",
    "        device=config.device,\n",
    "        diffusion_steps=config.diffusion_steps, \n",
    "        predict_noise=config.predict_noise)\n",
    "\n",
    "# loads agent\n",
    "agent.load('trained_models/DP-UNet-Uncond-Medium-decagon-8822f635diffusion_ckpt_200000.pt')\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "total_not_train = 0\n",
    "# sample language string\n",
    "sampled_trajectories = get_trajectory(agent,config,None)[0]\n",
    "for sample_trajectory in sampled_trajectories:\n",
    "    #sample_trajectory = sampled_trajectories[0][0]\n",
    "    # plots it on the map\n",
    "    #fig,ax = env.get_mpl_plot()\n",
    "    #for i,grid_point in enumerate(sample_trajectory):\n",
    "    #    plt.scatter(grid_point[0],grid_point[1],color=i*np.array((1,1,1))/512,zorder=-1)\n",
    "\n",
    "    # prints out the cooresponding plan string\n",
    "    landmark_tuple = env.get_trajectory_landmarks(sample_trajectory)\n",
    "    #print('Landmark Path:',landmark_tuple)\n",
    "    in_training = False\n",
    "    in_testing = False\n",
    "    for thing in training_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_training = True\n",
    "    for thing in testing_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_testing = True\n",
    "    #if not in_training:\n",
    "    #    print('Not In Training Data!')\n",
    "    if in_training:\n",
    "        total_train += 1\n",
    "    else:\n",
    "        total_not_train += 1\n",
    "    if in_testing:\n",
    "        total_test += 1\n",
    "    #for thing in testing_split:\n",
    "    #    if list(landmark_tuple) == list(thing['landmarks']):\n",
    "    #        print('In Testing Data!')\n",
    "print('# In Train:',total_train)\n",
    "print('# Not In Train:',total_not_train)\n",
    "print('# In Test:',total_test)\n",
    "print('Ratio:',total_not_train/(total_train+total_not_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d82c0cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# In Train: 232\n",
      "# Not In Train: 18\n",
      "# In Test: 2\n",
      "Ratio: 0.072\n"
     ]
    }
   ],
   "source": [
    "# U-Net unconditional, SMM\n",
    "\n",
    "training_split = np.load(save_directory + '/train_data_smm.npy',allow_pickle=True)\n",
    "config = DiffusionPlannerConfig()\n",
    "config.guidance = 'none'\n",
    "config.num_episodes = 1\n",
    "config.w_cfg = 1.0\n",
    "config.num_envs = num_envs\n",
    "obs_dim = 2\n",
    "fix_mask = torch.zeros((config.gen_horizon, obs_dim)) \n",
    "# --------------- Network Architecture -----------------\n",
    "if config.use_shift_equivariant_arch:\n",
    "    #nn_diffusion = ConvNext1dShiftEq(obs_dim)\n",
    "    nn_diffusion = CNN1dShiftEq(obs_dim,\n",
    "                                kernel_expansion_rate=config.kernel_expansion_rate,\n",
    "                                model_dim = config.model_dim,\n",
    "                                emb_dim = config.emb_dim,\n",
    "                                encode_position = config.add_positional_encoding)\n",
    "else:\n",
    "    nn_diffusion = JannerUNet1d(\n",
    "        obs_dim, model_dim=config.model_dim, emb_dim=config.model_dim, dim_mult=[1, 2, 2, 2],\n",
    "        timestep_emb_type=\"positional\", attention=config.self_attention, kernel_size=5,\n",
    "        use_timestep_emb=config.use_timestep_embeddings)\n",
    "\n",
    "\n",
    "classifier = None\n",
    "# creates condition network if needed (when using classifier-free guidance)\n",
    "if 'cfg' in config.guidance:\n",
    "    nn_condition = MLPCondition(\n",
    "    in_dim=config.lang_enc_size, out_dim=config.emb_dim, hidden_dims=[config.emb_dim, ], act=nn.SiLU(), dropout=config.label_dropout)\n",
    "else:\n",
    "    nn_condition = None\n",
    "loss_weight = torch.ones((config.gen_horizon, obs_dim))\n",
    "agent = DiscreteDiffusionSDE(\n",
    "        nn_diffusion = nn_diffusion, \n",
    "        nn_condition = nn_condition,\n",
    "        classifier = classifier, \n",
    "        fix_mask=fix_mask, \n",
    "        loss_weight=loss_weight, \n",
    "        ema_rate=config.ema_rate,\n",
    "        device=config.device,\n",
    "        diffusion_steps=config.diffusion_steps, \n",
    "        predict_noise=config.predict_noise)\n",
    "\n",
    "# loads agent\n",
    "agent.load('trained_models/DP-UNet-Uncond-SMM-decagon-e64cda3adiffusion_ckpt_200000.pt')\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "total_not_train = 0\n",
    "# sample language string\n",
    "sampled_trajectories = get_trajectory(agent,config,None)[0]\n",
    "for sample_trajectory in sampled_trajectories:\n",
    "    #sample_trajectory = sampled_trajectories[0][0]\n",
    "    # plots it on the map\n",
    "    #fig,ax = env.get_mpl_plot()\n",
    "    #for i,grid_point in enumerate(sample_trajectory):\n",
    "    #    plt.scatter(grid_point[0],grid_point[1],color=i*np.array((1,1,1))/512,zorder=-1)\n",
    "\n",
    "    # prints out the cooresponding plan string\n",
    "    landmark_tuple = env.get_trajectory_landmarks(sample_trajectory)\n",
    "    #print('Landmark Path:',landmark_tuple)\n",
    "    in_training = False\n",
    "    in_testing = False\n",
    "    for thing in training_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_training = True\n",
    "    for thing in testing_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_testing = True\n",
    "    #if not in_training:\n",
    "    #    print('Not In Training Data!')\n",
    "    if in_training:\n",
    "        total_train += 1\n",
    "    else:\n",
    "        total_not_train += 1\n",
    "    if in_testing:\n",
    "        total_test += 1\n",
    "    #for thing in testing_split:\n",
    "    #    if list(landmark_tuple) == list(thing['landmarks']):\n",
    "    #        print('In Testing Data!')\n",
    "print('# In Train:',total_train)\n",
    "print('# Not In Train:',total_not_train)\n",
    "print('# In Test:',total_test)\n",
    "print('Ratio:',total_not_train/(total_train+total_not_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "717cea58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# In Train: 248\n",
      "# Not In Train: 2\n",
      "# In Test: 0\n",
      "Ratio: 0.008\n"
     ]
    }
   ],
   "source": [
    "# U-Net unconditional, SM\n",
    "\n",
    "training_split = np.load(save_directory + '/train_data_sm.npy',allow_pickle=True)\n",
    "config = DiffusionPlannerConfig()\n",
    "config.guidance = 'none'\n",
    "config.num_episodes = 1\n",
    "config.w_cfg = 1.0\n",
    "config.num_envs = num_envs\n",
    "obs_dim = 2\n",
    "fix_mask = torch.zeros((config.gen_horizon, obs_dim)) \n",
    "# --------------- Network Architecture -----------------\n",
    "if config.use_shift_equivariant_arch:\n",
    "    #nn_diffusion = ConvNext1dShiftEq(obs_dim)\n",
    "    nn_diffusion = CNN1dShiftEq(obs_dim,\n",
    "                                kernel_expansion_rate=config.kernel_expansion_rate,\n",
    "                                model_dim = config.model_dim,\n",
    "                                emb_dim = config.emb_dim,\n",
    "                                encode_position = config.add_positional_encoding)\n",
    "else:\n",
    "    nn_diffusion = JannerUNet1d(\n",
    "        obs_dim, model_dim=config.model_dim, emb_dim=config.model_dim, dim_mult=[1, 2, 2, 2],\n",
    "        timestep_emb_type=\"positional\", attention=config.self_attention, kernel_size=5,\n",
    "        use_timestep_emb=config.use_timestep_embeddings)\n",
    "\n",
    "\n",
    "classifier = None\n",
    "# creates condition network if needed (when using classifier-free guidance)\n",
    "if 'cfg' in config.guidance:\n",
    "    nn_condition = MLPCondition(\n",
    "    in_dim=config.lang_enc_size, out_dim=config.emb_dim, hidden_dims=[config.emb_dim, ], act=nn.SiLU(), dropout=config.label_dropout)\n",
    "else:\n",
    "    nn_condition = None\n",
    "loss_weight = torch.ones((config.gen_horizon, obs_dim))\n",
    "agent = DiscreteDiffusionSDE(\n",
    "        nn_diffusion = nn_diffusion, \n",
    "        nn_condition = nn_condition,\n",
    "        classifier = classifier, \n",
    "        fix_mask=fix_mask, \n",
    "        loss_weight=loss_weight, \n",
    "        ema_rate=config.ema_rate,\n",
    "        device=config.device,\n",
    "        diffusion_steps=config.diffusion_steps, \n",
    "        predict_noise=config.predict_noise)\n",
    "\n",
    "# loads agent\n",
    "agent.load('trained_models/DP-UNet-Uncond-SM-decagon-f2faaa21diffusion_ckpt_latest.pt')\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "total_not_train = 0\n",
    "# sample language string\n",
    "sampled_trajectories = get_trajectory(agent,config,None)[0]\n",
    "for sample_trajectory in sampled_trajectories:\n",
    "    #sample_trajectory = sampled_trajectories[0][0]\n",
    "    # plots it on the map\n",
    "    #fig,ax = env.get_mpl_plot()\n",
    "    #for i,grid_point in enumerate(sample_trajectory):\n",
    "    #    plt.scatter(grid_point[0],grid_point[1],color=i*np.array((1,1,1))/512,zorder=-1)\n",
    "\n",
    "    # prints out the cooresponding plan string\n",
    "    landmark_tuple = env.get_trajectory_landmarks(sample_trajectory)\n",
    "    #print('Landmark Path:',landmark_tuple)\n",
    "    in_training = False\n",
    "    in_testing = False\n",
    "    for thing in training_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_training = True\n",
    "    for thing in testing_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_testing = True\n",
    "    #if not in_training:\n",
    "    #    print('Not In Training Data!')\n",
    "    if in_training:\n",
    "        total_train += 1\n",
    "    else:\n",
    "        total_not_train += 1\n",
    "    if in_testing:\n",
    "        total_test += 1\n",
    "    #for thing in testing_split:\n",
    "    #    if list(landmark_tuple) == list(thing['landmarks']):\n",
    "    #        print('In Testing Data!')\n",
    "print('# In Train:',total_train)\n",
    "print('# Not In Train:',total_not_train)\n",
    "print('# In Test:',total_test)\n",
    "print('Ratio:',total_not_train/(total_train+total_not_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30b57b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# In Train: 250\n",
      "# Not In Train: 0\n",
      "# In Test: 0\n",
      "Ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    "# U-Net unconditional, SSM\n",
    "\n",
    "training_split = np.load(save_directory + '/train_data_ssm.npy',allow_pickle=True)\n",
    "config = DiffusionPlannerConfig()\n",
    "config.guidance = 'none'\n",
    "config.num_episodes = 1\n",
    "config.w_cfg = 1.0\n",
    "config.num_envs = num_envs\n",
    "obs_dim = 2\n",
    "fix_mask = torch.zeros((config.gen_horizon, obs_dim)) \n",
    "# --------------- Network Architecture -----------------\n",
    "if config.use_shift_equivariant_arch:\n",
    "    #nn_diffusion = ConvNext1dShiftEq(obs_dim)\n",
    "    nn_diffusion = CNN1dShiftEq(obs_dim,\n",
    "                                kernel_expansion_rate=config.kernel_expansion_rate,\n",
    "                                model_dim = config.model_dim,\n",
    "                                emb_dim = config.emb_dim,\n",
    "                                encode_position = config.add_positional_encoding)\n",
    "else:\n",
    "    nn_diffusion = JannerUNet1d(\n",
    "        obs_dim, model_dim=config.model_dim, emb_dim=config.model_dim, dim_mult=[1, 2, 2, 2],\n",
    "        timestep_emb_type=\"positional\", attention=config.self_attention, kernel_size=5,\n",
    "        use_timestep_emb=config.use_timestep_embeddings)\n",
    "\n",
    "\n",
    "classifier = None\n",
    "# creates condition network if needed (when using classifier-free guidance)\n",
    "if 'cfg' in config.guidance:\n",
    "    nn_condition = MLPCondition(\n",
    "    in_dim=config.lang_enc_size, out_dim=config.emb_dim, hidden_dims=[config.emb_dim, ], act=nn.SiLU(), dropout=config.label_dropout)\n",
    "else:\n",
    "    nn_condition = None\n",
    "loss_weight = torch.ones((config.gen_horizon, obs_dim))\n",
    "agent = DiscreteDiffusionSDE(\n",
    "        nn_diffusion = nn_diffusion, \n",
    "        nn_condition = nn_condition,\n",
    "        classifier = classifier, \n",
    "        fix_mask=fix_mask, \n",
    "        loss_weight=loss_weight, \n",
    "        ema_rate=config.ema_rate,\n",
    "        device=config.device,\n",
    "        diffusion_steps=config.diffusion_steps, \n",
    "        predict_noise=config.predict_noise)\n",
    "\n",
    "# loads agent\n",
    "agent.load('trained_models/DP-UNet-Uncond-SSM-decagon-c590c495diffusion_ckpt_200000.pt')\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "total_not_train = 0\n",
    "# sample language string\n",
    "sampled_trajectories = get_trajectory(agent,config,None)[0]\n",
    "for sample_trajectory in sampled_trajectories:\n",
    "    #sample_trajectory = sampled_trajectories[0][0]\n",
    "    # plots it on the map\n",
    "    #fig,ax = env.get_mpl_plot()\n",
    "    #for i,grid_point in enumerate(sample_trajectory):\n",
    "    #    plt.scatter(grid_point[0],grid_point[1],color=i*np.array((1,1,1))/512,zorder=-1)\n",
    "\n",
    "    # prints out the cooresponding plan string\n",
    "    landmark_tuple = env.get_trajectory_landmarks(sample_trajectory)\n",
    "    #print('Landmark Path:',landmark_tuple)\n",
    "    in_training = False\n",
    "    in_testing = False\n",
    "    for thing in training_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_training = True\n",
    "    for thing in testing_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_testing = True\n",
    "    #if not in_training:\n",
    "    #    print('Not In Training Data!')\n",
    "    if in_training:\n",
    "        total_train += 1\n",
    "    else:\n",
    "        total_not_train += 1\n",
    "    if in_testing:\n",
    "        total_test += 1\n",
    "    #for thing in testing_split:\n",
    "    #    if list(landmark_tuple) == list(thing['landmarks']):\n",
    "    #        print('In Testing Data!')\n",
    "print('# In Train:',total_train)\n",
    "print('# Not In Train:',total_not_train)\n",
    "print('# In Test:',total_test)\n",
    "print('Ratio:',total_not_train/(total_train+total_not_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f15ba0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# In Train: 250\n",
      "# Not In Train: 0\n",
      "# In Test: 0\n",
      "Ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    "# U-Net unconditional, small\n",
    "\n",
    "training_split = np.load(save_directory + '/train_data_small.npy',allow_pickle=True)\n",
    "config = DiffusionPlannerConfig()\n",
    "config.guidance = 'none'\n",
    "config.num_episodes = 1\n",
    "config.w_cfg = 1.0\n",
    "config.num_envs = num_envs\n",
    "obs_dim = 2\n",
    "fix_mask = torch.zeros((config.gen_horizon, obs_dim)) \n",
    "# --------------- Network Architecture -----------------\n",
    "if config.use_shift_equivariant_arch:\n",
    "    #nn_diffusion = ConvNext1dShiftEq(obs_dim)\n",
    "    nn_diffusion = CNN1dShiftEq(obs_dim,\n",
    "                                kernel_expansion_rate=config.kernel_expansion_rate,\n",
    "                                model_dim = config.model_dim,\n",
    "                                emb_dim = config.emb_dim,\n",
    "                                encode_position = config.add_positional_encoding)\n",
    "else:\n",
    "    nn_diffusion = JannerUNet1d(\n",
    "        obs_dim, model_dim=config.model_dim, emb_dim=config.model_dim, dim_mult=[1, 2, 2, 2],\n",
    "        timestep_emb_type=\"positional\", attention=config.self_attention, kernel_size=5,\n",
    "        use_timestep_emb=config.use_timestep_embeddings)\n",
    "\n",
    "\n",
    "classifier = None\n",
    "# creates condition network if needed (when using classifier-free guidance)\n",
    "if 'cfg' in config.guidance:\n",
    "    nn_condition = MLPCondition(\n",
    "    in_dim=config.lang_enc_size, out_dim=config.emb_dim, hidden_dims=[config.emb_dim, ], act=nn.SiLU(), dropout=config.label_dropout)\n",
    "else:\n",
    "    nn_condition = None\n",
    "loss_weight = torch.ones((config.gen_horizon, obs_dim))\n",
    "agent = DiscreteDiffusionSDE(\n",
    "        nn_diffusion = nn_diffusion, \n",
    "        nn_condition = nn_condition,\n",
    "        classifier = classifier, \n",
    "        fix_mask=fix_mask, \n",
    "        loss_weight=loss_weight, \n",
    "        ema_rate=config.ema_rate,\n",
    "        device=config.device,\n",
    "        diffusion_steps=config.diffusion_steps, \n",
    "        predict_noise=config.predict_noise)\n",
    "\n",
    "# loads agent\n",
    "agent.load('trained_models/DP-UNet-Uncond-Small-decagon-c400557ediffusion_ckpt_200000.pt')\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "total_not_train = 0\n",
    "# sample language string\n",
    "sampled_trajectories = get_trajectory(agent,config,None)[0]\n",
    "for sample_trajectory in sampled_trajectories:\n",
    "    #sample_trajectory = sampled_trajectories[0][0]\n",
    "    # plots it on the map\n",
    "    #fig,ax = env.get_mpl_plot()\n",
    "    #for i,grid_point in enumerate(sample_trajectory):\n",
    "    #    plt.scatter(grid_point[0],grid_point[1],color=i*np.array((1,1,1))/512,zorder=-1)\n",
    "\n",
    "    # prints out the cooresponding plan string\n",
    "    landmark_tuple = env.get_trajectory_landmarks(sample_trajectory)\n",
    "    #print('Landmark Path:',landmark_tuple)\n",
    "    in_training = False\n",
    "    in_testing = False\n",
    "    for thing in training_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_training = True\n",
    "    for thing in testing_split:\n",
    "        if list(landmark_tuple) == list(thing['landmarks']):\n",
    "            #print('In Training Data!')\n",
    "            in_testing = True\n",
    "    #if not in_training:\n",
    "    #    print('Not In Training Data!')\n",
    "    if in_training:\n",
    "        total_train += 1\n",
    "    else:\n",
    "        total_not_train += 1\n",
    "    if in_testing:\n",
    "        total_test += 1\n",
    "    #for thing in testing_split:\n",
    "    #    if list(landmark_tuple) == list(thing['landmarks']):\n",
    "    #        print('In Testing Data!')\n",
    "print('# In Train:',total_train)\n",
    "print('# Not In Train:',total_not_train)\n",
    "print('# In Test:',total_test)\n",
    "print('Ratio:',total_not_train/(total_train+total_not_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_stitch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
