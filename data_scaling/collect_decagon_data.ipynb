{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877be312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30240/30240 [14:01<00:00, 35.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# gets the environment set up \n",
    "from utilities.decagon_env import Decagon\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sets up env\n",
    "env = Decagon()\n",
    "HOLDOUT_SPLIT = 0.2\n",
    "RANDOM_SEED = 42\n",
    "tolerance = 0.01\n",
    "gaussian_scale = 0.05 # random noise during action collection\n",
    "save_directory = 'decagon_data'\n",
    "\n",
    "# generates the landmark tuples\n",
    "landmarks = list(env.point_dict.keys())\n",
    "landmark_tuples = []\n",
    "for landmark_1 in landmarks:\n",
    "    for landmark_2 in landmarks:\n",
    "        for landmark_3 in landmarks:\n",
    "            for landmark_4 in landmarks:\n",
    "                for landmark_5 in landmarks:\n",
    "                    if len(set((landmark_1,landmark_2,landmark_3,landmark_4,landmark_5))) == 5:\n",
    "                        landmark_tuples.append(tuple((landmark_1,landmark_2,landmark_3,landmark_4,landmark_5)))\n",
    "\n",
    "# shuffles list and generates train/test split\n",
    "rng = np.random.default_rng(seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "# creates storing stuff \n",
    "trajectory_storage = [] # list of trajectories, each of which has state observations and matching string\n",
    "skip_1 = 0\n",
    "skip_2 = 0\n",
    "# for each tuple generates a sequence\n",
    "for landmark_tuple in tqdm(landmark_tuples):\n",
    "    storage_dict = {}\n",
    "\n",
    "    # state storage\n",
    "    state_storage = []\n",
    "    # resets env with approrpriate goal\n",
    "    state, _ = env.reset(options={'start_point':landmark_tuple[0]})\n",
    "    landmark_index = 1\n",
    "    at_next_landmark = False\n",
    "    next_landmark = landmark_tuple[landmark_index]\n",
    "    landmark_coords = env.point_dict[next_landmark]\n",
    "    # storage for plan sketcher\n",
    "    landmark_storage = [landmark_tuple[0]]\n",
    "    distance_storage = []\n",
    "    distance = 0\n",
    "    completed = False\n",
    "    # main loop\n",
    "    for i in range(512):\n",
    "        state_storage.append(state)\n",
    "        action = landmark_coords - state\n",
    "        action = action + rng.normal(loc=0,scale=gaussian_scale,size=action.shape)\n",
    "        state, _, terminated, truncated, info = env.step(action)\n",
    "        at_next_landmark = np.linalg.norm(state-landmark_coords) < tolerance\n",
    "        distance += 1\n",
    "        if at_next_landmark:\n",
    "            # plan sketcher storage\n",
    "            if not completed:\n",
    "                landmark_storage.append(landmark_tuple[landmark_index])\n",
    "                distance_storage.append(distance)\n",
    "                distance = 0\n",
    "\n",
    "            landmark_index += 1\n",
    "            if landmark_index > 4:\n",
    "                landmark_index = 4\n",
    "                completed = True\n",
    "            at_next_landmark = False\n",
    "            next_landmark = landmark_tuple[landmark_index]\n",
    "            landmark_coords = env.point_dict[next_landmark]\n",
    "\n",
    "            \n",
    "    if len(landmark_storage) == 3: # for cases where last 2 landmarks are missed - should be rare\n",
    "        skip_2 += 1\n",
    "    if len(landmark_storage) == 4: # for cases where last landmark was missed\n",
    "        skip_1 += 1 \n",
    "    \n",
    "\n",
    "    storage_dict.update({\n",
    "                         'trajectory':np.array(state_storage),\n",
    "                         'landmarks': np.array(landmark_storage),\n",
    "                         'distances': np.array(distance_storage)\n",
    "                         })\n",
    "    trajectory_storage.append(storage_dict)\n",
    "\n",
    "\n",
    "# training/testing splits\n",
    "rng.shuffle(trajectory_storage)\n",
    "training_split = trajectory_storage[:1-int(HOLDOUT_SPLIT*len(trajectory_storage))]\n",
    "testing_split = trajectory_storage[1-int(HOLDOUT_SPLIT*len(trajectory_storage)):]\n",
    "# saves stuff\n",
    "\n",
    "\n",
    "print(skip_2)\n",
    "print(skip_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61d68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(save_directory + '/train_data',training_split)\n",
    "np.save(save_directory + '/test_data',testing_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15cb89db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24193\n"
     ]
    }
   ],
   "source": [
    "# making smaller data splits\n",
    "\n",
    "save_directory = 'decagon_data'\n",
    "\n",
    "\n",
    "training_split = np.load(save_directory + '/train_data.npy',allow_pickle=True)\n",
    "print(len(training_split))\n",
    "training_split_medium = training_split[:2500]\n",
    "training_split_small = training_split[:250]\n",
    "training_split_tiny = training_split[:25]\n",
    "\n",
    "np.save(save_directory + '/train_data_medium',training_split_medium)\n",
    "np.save(save_directory + '/train_data_small',training_split_small)\n",
    "np.save(save_directory + '/train_data_tiny',training_split_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21024c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24193\n"
     ]
    }
   ],
   "source": [
    "# weirder data splits\n",
    "import numpy as np\n",
    "save_directory = 'decagon_data'\n",
    "training_split = np.load(save_directory + '/train_data.npy',allow_pickle=True)\n",
    "print(len(training_split))\n",
    "training_split_ssm = training_split[:444]\n",
    "training_split_sm = training_split[:790]\n",
    "training_split_smm = training_split[:1405]\n",
    "\n",
    "np.save(save_directory + '/train_data_ssm',training_split_ssm)\n",
    "np.save(save_directory + '/train_data_sm',training_split_sm)\n",
    "np.save(save_directory + '/train_data_smm',training_split_smm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
